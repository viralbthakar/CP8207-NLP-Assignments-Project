{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy-based Knowledge Graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a knowledge graph?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A knowledge graph is a data structure. This graph is a way to represent information embedded within text in ///write more here!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic knowledge graph would be (node)----edge---->(node), or more specifically (entity A)----predicate---->(entity B), where predicate is a *relationship* from entity A aka the *subject* to entity B aka the *object*.<br> \n",
    "The usage of *relationship* is important, as you could have a triple like <br>\n",
    "<br>\n",
    "    (Dean Pelton)----loves---->(Jeff)<br><br>\n",
    "    where the predicate is a verb (this is called a data graph), but <br> <br>\n",
    "    (Metaphysics)----subclass---->(Philosophy)<br> <br>shows a *relationship* not in terms of a verb, but in terms of a taxonomy!\n",
    "One could begin to realize the possibilities of organizing data with this structure: <br><br>\n",
    "For example....<br><br>\n",
    "(Dean Pelton)----loves---->(Jeff)*----studies---->*(Metaphysics)----subclass---->(Philosophy)\n",
    "<br><br>\n",
    "Once a subject has multiple edges, one can begin to imagine the *power* that a knowledge graph can weild!\n",
    "<br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above writing, let's begin with developing a naive, simple methodology with respect to extracting a triple (aka node---predicate--->node) from a text document"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For a given document:\n",
    "2. Find the entities in the document(using spaCy)\n",
    "3. Find the relationship between each entity in the document"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sounds so easy! Right? Of course not!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Find the entities in the document\n",
    "How does spacy find entities though?\n",
    "I'm not sure *exactly* how spaCy may do so, however, according to [this Microsoft blog post](https://learn.microsoft.com/en-us/archive/blogs/machinelearning/machine-learning-and-text-analytics), it may train some ML model with text which has its named entities annotated with a tag. Maybe the text could be chunked by sentence, and tagged with its POS tag(noun, verb, adj, etc) to provide more features to the model.<br>\n",
    "I kinda lied. spaCy does so with a CNN. However, I am unsure about the more specific information regarding this process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think it's important to begin doing some actual coding! So let's just run with the naive approach and try to extract entites from some text with spaCy to begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp_model = spacy.load(\"en_core_web_sm\") \n",
    "\"\"\"spacy.load will \"load\" a pretrained\n",
    " model which can perform tokenization, \n",
    " POS tagging, depency parsing, NER, text categorizer(sentiment, spam),\n",
    " rule based matcher(like regex), entity linker, more. In our case the \n",
    " en_core_web_sm(english core pipieline features, trained on web, small) has \n",
    " tok2vec, tagger, parser, attribute_ruler, lemmatizer, ner\"\"\"\n",
    "\n",
    "\n",
    "our_text = \"Jessica M Roberts lives in Canada\"\n",
    "doc = nlp_model(our_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily spaCy makes things pretty readable, so I think this code below should speak for itself! But in case it doesn't, we are just taking a string, running it through our pretrained model for named entity extraction(NER), and mapping the entity's name to it's entity type!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_NER(input_str):\n",
    "    annotated_str = {}\n",
    "    for entity in nlp_model(input_str).ents:\n",
    "        annotated_str[entity.text] = entity.label_\n",
    "    return annotated_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Jessica M Roberts': 'PERSON', 'Canada': 'GPE'}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_NER(our_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to recognize that if we search for an index with a spaCy token doc object, we will get a token object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jessica M Roberts lives in Canada\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "Jessica\n",
      "<class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "print(doc)\n",
    "print(type(doc))\n",
    "print(doc[0])\n",
    "print(type(doc[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a *head* attribute to each token within a doc object. This head(token A) is itself a token and acts is seen as the syntactic parent of token B. \n",
    "\n",
    "The root of a sentence is a token which matches its head!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Jessica: Roberts, M: Roberts, Roberts: lives, lives: lives, in: lives, Canada: in}\n",
      "The root of this sentence is:  [lives]\n"
     ]
    }
   ],
   "source": [
    "heads = {x:x.head for x in doc}\n",
    "print(heads)\n",
    "root = [x for x in heads if heads[x]==x]\n",
    "print(\"The root of this sentence is: \",root)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule Based relation extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's important to recognize how this simple sentence works in terms of a knowledge graph's triple. (Jessica Roberts)---Lives in---->(Canada). So it's structured as `(COMPOUND NOUN NOUN-SUBJECT) (ROOT-VERB) (PREPOSITION NOUN-OBJECT)`. Assuming that with every sentence we have, there will be a subject acting on an object , maybe we can extract the relationship between the two named entities with a rule"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This rule will look like:\n",
    "for token in sentence:\n",
    "    if compound\n",
    "(COMPOUND-subject-noun)----VERB-ROOT and PREPOSITION-----(entity-object-noun)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subject': {'Jessica': 0, 'M': 8, 'Roberts': 10}, 'object': {'Canada': 27}, 'predicate': {'lives': 18, 'in': 24}}\n"
     ]
    }
   ],
   "source": [
    "def subject_object_predicate(doc, entities):\n",
    "    subject = {}\n",
    "    object = {}\n",
    "    predicate = {}\n",
    "    for token_n, token in enumerate(doc):\n",
    "        #This will lead to compounds from one entity to be in the other! But we can clean those out later\n",
    "        if \"compound\" in token.dep_:\n",
    "            subject[token.text] = token.idx\n",
    "        if \"subj\" in token.dep_:\n",
    "            subject[token.text] = token.idx\n",
    "        \n",
    "        #This will lead to compounds from one entity to be in the other! But we can clean those out later\n",
    "        if \"compound\" in token.dep_:\n",
    "            object[token.text] = token.idx\n",
    "        if \"obj\" in token.dep_:\n",
    "            object[token.text] = token.idx\n",
    "\n",
    "\n",
    "        if \"verb\" in token.pos_.lower() or \"root\" in token.dep_.lower():\n",
    "            predicate[token.text] = token.idx\n",
    "            if \"prep\" in doc[token_n+1].dep_:\n",
    "                predicate[doc[token_n+1].text] = doc[token_n+1].idx\n",
    "    \n",
    "    #clean object and subject \n",
    "    #For each entity\n",
    "    for entity in entities:\n",
    "        #Split entity into a list\n",
    "        entity_list = entity.split(\" \")\n",
    "        #For the object, find which words in it match the current entity\n",
    "        object_match = [x for x in list(object.keys()) if x in entity_list]\n",
    "        #If it's not a perfect match, then the object has compounds in it that don't exist in the actual entity it's associated with! So we should remove those matching words\n",
    "        if entity_list != object_match:\n",
    "            for word in object_match:\n",
    "                object.pop(word)\n",
    "        #For the subject, find which words in it match the current entity\n",
    "        subject_match = [x for x in list(subject.keys()) if x in entity_list]\n",
    "        #If it's not a perfect match, then the subject has compounds in it that don't exist in the actual entity it's associated with! So we should remove those matching words\n",
    "        if entity_list != subject_match:\n",
    "            for word in subject_match:\n",
    "                object.pop(word)\n",
    "    return {\"subject\":subject, \"object\":object, \"predicate\":predicate}\n",
    "print(subject_object_predicate(nlp_model(our_text), entities = spacy_NER(our_text)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get a rule based triple from our sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Jessica M Roberts)---(lives in)--->(Canada)\n"
     ]
    }
   ],
   "source": [
    "triple = subject_object_predicate(nlp_model(our_text), \n",
    "    entities = spacy_NER(our_text))\n",
    "print(f\"({' '.join(list(triple['subject']))})---({' '.join(list(triple['predicate']))})--->({' '.join(list(triple['object']))})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should make this into a pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Jessica M Roberts)---(is from)--->(Canada)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'subject': {'Jessica': 0, 'M': 8, 'Roberts': 10},\n",
       " 'object': {'Canada': 26},\n",
       " 'predicate': {'is': 18, 'from': 21}}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def spacyRule_triple_extraction_pipeline(sentence):\n",
    "    doc = nlp_model(sentence)\n",
    "    entities = spacy_NER(sentence)\n",
    "    triple = subject_object_predicate(doc, entities)\n",
    "    print(f\"({' '.join(list(triple['subject']))})---({' '.join(list(triple['predicate']))})--->({' '.join(list(triple['object']))})\")\n",
    "    return subject_object_predicate(doc, entities)\n",
    "spacyRule_triple_extraction_pipeline(\"Jessica M Roberts is from Canada\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPENIE based knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Barack Obama was born in Hawaii. Richard Manning wrote this sentence..\n",
      "Starting server with command: java -Xmx8G -cp /home/monty/.stanfordnlp_resources/stanford-corenlp-4.1.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-0b085fd4193e49e3.props -preload openie\n"
     ]
    },
    {
     "ename": "PermanentlyFailedException",
     "evalue": "Timed out waiting for service to come alive.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermanentlyFailedException\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_420193/148240852.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Barack Obama was born in Hawaii. Richard Manning wrote this sentence.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Text: %s.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtriple\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'|-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/openie/openie.py\u001b[0m in \u001b[0;36mannotate\u001b[0;34m(self, text, properties_key, properties, simple_format)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \"\"\"\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# https://stanfordnlp.github.io/CoreNLP/openie.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         core_nlp_output = self.client.annotate(text=text, annotators=['openie'], output_format='json',\n\u001b[0m\u001b[1;32m     56\u001b[0m                                                properties_key=properties_key, properties=properties)\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msimple_format\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/stanfordnlp/server/client.py\u001b[0m in \u001b[0;36mannotate\u001b[0;34m(self, text, annotators, output_format, properties_key, properties, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0mrequest_properties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'outputFormat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoreNLPClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_OUTPUT_FORMAT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;31m# make the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_properties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequest_properties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"outputFormat\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"json\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/stanfordnlp/server/client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, buf, properties, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrequest\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \"\"\"\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/stanfordnlp/server/client.py\u001b[0m in \u001b[0;36mensure_alive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mPermanentlyFailedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Timed out waiting for service to come alive.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# At this point we are guaranteed that the service is alive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermanentlyFailedException\u001b[0m: Timed out waiting for service to come alive."
     ]
    }
   ],
   "source": [
    "from openie import StanfordOpenIE\n",
    "\n",
    "# https://stanfordnlp.github.io/CoreNLP/openie.html#api\n",
    "# Default value of openie.affinity_probability_cap was 1/3.\n",
    "properties = {\n",
    "    'openie.affinity_probability_cap': 2 / 3,\n",
    "}\n",
    "\n",
    "with StanfordOpenIE(properties=properties) as client:\n",
    "    text = 'Barack Obama was born in Hawaii. Richard Manning wrote this sentence.'\n",
    "    print('Text: %s.' % text)\n",
    "    for triple in client.annotate(text):\n",
    "        print('|-', triple)\n",
    "\n",
    "    graph_image = 'graph.png'\n",
    "    client.generate_graphviz_graph(text, graph_image)\n",
    "    print('Graph generated: %s.' % graph_image)\n",
    "\n",
    "    with open('corpus/pg6130.txt', encoding='utf8') as r:\n",
    "        corpus = r.read().replace('\\n', ' ').replace('\\r', '')\n",
    "\n",
    "    triples_corpus = client.annotate(corpus[0:5000])\n",
    "    print('Corpus: %s [...].' % corpus[0:80])\n",
    "    print('Found %s triples in the corpus.' % len(triples_corpus))\n",
    "    for triple in triples_corpus[:3]:\n",
    "        print('|-', triple)\n",
    "    print('[...]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM-based NER"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GOAL: Train a Many-Many LSTM with a word, and its respective POS tag. Each will have an output as O or {B-art', 'B-eve', 'B-geo', 'B-gpe', 'B-nat', 'B-org', 'B-per','B-tim', 'I-art', 'I-eve', 'I-geo', 'I-gpe', 'I-nat', 'I-org','I-per', 'I-tim',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"ner_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048570</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>they</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048571</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>responded</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048572</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048573</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048574</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>attack</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1048575 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Sentence #           Word  POS Tag\n",
       "0            Sentence: 1      Thousands  NNS   O\n",
       "1            Sentence: 1             of   IN   O\n",
       "2            Sentence: 1  demonstrators  NNS   O\n",
       "3            Sentence: 1           have  VBP   O\n",
       "4            Sentence: 1        marched  VBN   O\n",
       "...                  ...            ...  ...  ..\n",
       "1048570  Sentence: 47959           they  PRP   O\n",
       "1048571  Sentence: 47959      responded  VBD   O\n",
       "1048572  Sentence: 47959             to   TO   O\n",
       "1048573  Sentence: 47959            the   DT   O\n",
       "1048574  Sentence: 47959         attack   NN   O\n",
       "\n",
       "[1048575 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fillna = data.fillna(method='ffill', axis=0)\n",
    "data_fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_map(data, token_or_tag):\n",
    "    tok2idx = {}\n",
    "    idx2tok = {}\n",
    "    \n",
    "    if token_or_tag == 'token':\n",
    "        vocab = list(set(data['Word'].to_list()))\n",
    "    else:\n",
    "        vocab = list(set(data['Tag'].to_list()))\n",
    "    \n",
    "    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}\n",
    "    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}\n",
    "    return tok2idx, idx2tok\n",
    "\n",
    "\n",
    "token2idx, idx2token = get_dict_map(data, 'token')\n",
    "tag2idx, idx2tag = get_dict_map(data, 'tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Word_idx'] = data['Word'].map(token2idx)\n",
    "data['Tag_idx'] = data['Tag'].map(tag2idx) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put each sentence into a single cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_328642/3141371824.py:2: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  data_group = data_fillna.groupby(['Sentence #'],as_index=False)['Word', 'POS', 'Tag', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))\n"
     ]
    }
   ],
   "source": [
    "data_fillna = data.fillna(method='ffill', axis=0)\n",
    "data_group = data_fillna.groupby(['Sentence #'],as_index=False)['Word', 'POS', 'Tag', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence #                                         Sentence: 10\n",
       "Word          [Iranian, officials, say, they, expect, to, ge...\n",
       "POS           [JJ, NNS, VBP, PRP, VBP, TO, VB, NN, TO, JJ, J...\n",
       "Tag           [B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...\n",
       "Word_idx      [34060, 34845, 30973, 28080, 14747, 15710, 176...\n",
       "Tag_idx       [15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 1...\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_group.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B-art', 'B-eve', 'B-geo', 'B-gpe', 'B-nat', 'B-org', 'B-per',\n",
       "       'B-tim', 'I-art', 'I-eve', 'I-geo', 'I-gpe', 'I-nat', 'I-org',\n",
       "       'I-per', 'I-tim', 'O'], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(data[\"Tag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(list(set(data['Word'].to_list())))+1\n",
    "input_length = max(len(x) for x in data_group[\"Word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21452,\n",
       " 9016,\n",
       " 34659,\n",
       " 11975,\n",
       " 22112,\n",
       " 12369,\n",
       " 15362,\n",
       " 15710,\n",
       " 10922,\n",
       " 30879,\n",
       " 34158,\n",
       " 2799,\n",
       " 2096,\n",
       " 24391,\n",
       " 21519,\n",
       " 30879,\n",
       " 33679,\n",
       " 9016,\n",
       " 5748,\n",
       " 30214,\n",
       " 21642,\n",
       " 7434,\n",
       " 11627,\n",
       " 28853]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_group.iloc[0][\"Word_idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "    # Add Embedding layer\n",
    "model.add(Embedding(input_dim=input_dim, output_dim=32, input_length=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"SGD\", loss=\"msr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-3.60159501e-02,  3.03565897e-02,  5.51499054e-03,\n",
       "        -2.10718997e-02, -4.46258895e-02, -4.94754910e-02,\n",
       "         3.57830413e-02,  2.52403729e-02,  2.11687796e-02,\n",
       "        -3.88580933e-02,  2.62679197e-02, -1.38202682e-02,\n",
       "        -4.95600700e-02,  4.11613248e-02, -1.14636309e-02,\n",
       "         4.68579419e-02,  1.36339925e-02, -3.11755296e-02,\n",
       "         2.78808735e-02,  1.63600780e-02, -1.64666548e-02,\n",
       "         2.96495594e-02,  1.63764097e-02,  4.05922048e-02,\n",
       "        -1.01650842e-02,  4.45184000e-02, -4.95735668e-02,\n",
       "        -4.19676192e-02,  4.36299555e-02,  2.13460959e-02,\n",
       "        -3.67555730e-02, -2.49840263e-02],\n",
       "       [ 4.58573438e-02,  4.57807668e-02, -1.88681372e-02,\n",
       "         3.49065922e-02,  2.17909105e-02,  3.73826958e-02,\n",
       "         1.75218657e-03, -2.76631005e-02,  4.21754271e-03,\n",
       "         1.85655467e-02,  4.39696386e-03, -4.81691025e-02,\n",
       "         7.42132589e-03,  4.30054925e-02, -2.82822382e-02,\n",
       "         3.03554796e-02,  4.08295654e-02,  3.11111473e-02,\n",
       "         9.20949131e-03,  4.01060320e-02, -1.16412155e-02,\n",
       "        -5.81056997e-03,  2.25255825e-02, -1.33761019e-03,\n",
       "         2.72017829e-02, -3.99578921e-02,  4.13048267e-03,\n",
       "        -7.86959007e-03,  1.45059712e-02,  1.88225247e-02,\n",
       "        -1.65681466e-02,  4.82491404e-03],\n",
       "       [-2.91140210e-02, -4.34620269e-02,  2.03279145e-02,\n",
       "         3.58084552e-02,  1.52307637e-02,  3.68422307e-02,\n",
       "         4.29418199e-02,  7.73922354e-03,  1.08282678e-02,\n",
       "        -6.57187775e-03,  4.30714600e-02, -2.95332819e-03,\n",
       "        -1.86302885e-02,  3.93212922e-02,  1.63844116e-02,\n",
       "         2.21236013e-02,  2.44956650e-02, -1.50961503e-02,\n",
       "         3.36492918e-02,  4.69271429e-02,  9.55256075e-03,\n",
       "        -4.58190814e-02, -4.87592481e-02, -1.00749508e-02,\n",
       "         4.49445099e-03,  4.13081683e-02,  2.92486213e-02,\n",
       "        -2.53113154e-02,  5.32995909e-04, -3.40550169e-02,\n",
       "        -1.93872936e-02, -2.09569335e-02],\n",
       "       [ 2.47022025e-02,  1.45045854e-02,  1.16804950e-02,\n",
       "         3.69576551e-02, -4.05425206e-02, -9.56274569e-04,\n",
       "        -1.85755640e-03,  2.39878893e-03, -4.90391031e-02,\n",
       "        -1.67669170e-02, -1.25574842e-02,  4.81983311e-02,\n",
       "         4.42564487e-03, -2.51399763e-02, -2.67850999e-02,\n",
       "        -2.76694447e-03, -4.38305140e-02, -1.65757537e-02,\n",
       "         8.90444964e-04,  3.60011719e-02,  4.43170927e-02,\n",
       "        -3.46747786e-03,  4.31133397e-02,  2.02207007e-02,\n",
       "         2.74343379e-02, -2.34974548e-03, -3.54807377e-02,\n",
       "        -1.21353045e-02,  1.42055489e-02,  4.50756401e-03,\n",
       "        -7.40064308e-03, -2.81387568e-02],\n",
       "       [-1.37996785e-02,  1.90427154e-03, -7.29221106e-03,\n",
       "        -4.98476513e-02, -3.64383459e-02,  4.38041352e-02,\n",
       "        -5.00531122e-03, -3.98444645e-02,  1.56128891e-02,\n",
       "        -2.28283759e-02, -2.21825596e-02, -2.88394224e-02,\n",
       "        -3.74137647e-02, -1.98573954e-02, -7.82482699e-03,\n",
       "         4.00710814e-02,  4.16603200e-02, -6.22211769e-03,\n",
       "         4.34959419e-02, -4.55472618e-03, -2.82737743e-02,\n",
       "        -2.79397964e-02, -5.24475425e-03, -4.62037921e-02,\n",
       "         1.04826316e-02, -2.10794210e-02,  3.82073186e-02,\n",
       "        -1.01264566e-03,  2.69970559e-02, -2.86226030e-02,\n",
       "         1.71777941e-02,  1.62035488e-02],\n",
       "       [ 1.95259340e-02, -3.56252193e-02, -3.45194116e-02,\n",
       "        -3.81317511e-02, -2.58335229e-02,  1.35624297e-02,\n",
       "         3.25813033e-02,  1.74140446e-02,  2.76385657e-02,\n",
       "         3.40543427e-02,  3.26543115e-02,  2.00075172e-02,\n",
       "        -2.75858529e-02,  4.21539061e-02, -1.87978148e-05,\n",
       "         3.67672332e-02,  4.80153412e-03,  1.51099004e-02,\n",
       "         3.52188386e-02, -1.42682791e-02,  4.50088717e-02,\n",
       "         1.52004883e-03, -1.30146258e-02, -1.10260472e-02,\n",
       "         4.08904888e-02, -4.05590422e-02, -4.41459902e-02,\n",
       "         8.26581568e-03,  2.99387835e-02,  2.20591761e-02,\n",
       "         4.18118574e-02, -1.28309838e-02],\n",
       "       [-3.01667452e-02,  1.29978918e-02, -7.96377659e-03,\n",
       "         1.42537244e-02,  2.09933557e-02, -3.74840870e-02,\n",
       "         4.33030762e-02,  2.05248632e-02, -8.54400545e-03,\n",
       "        -4.24099341e-02,  2.84926035e-02, -4.32972088e-02,\n",
       "         1.07172020e-02,  4.04441245e-02,  9.79231670e-03,\n",
       "        -2.25741994e-02, -1.34296417e-02,  2.94895098e-03,\n",
       "         1.07941628e-02, -1.58976540e-02, -8.48799944e-03,\n",
       "         3.28772105e-02, -2.77809631e-02,  4.00281288e-02,\n",
       "         3.62914316e-02, -3.56300920e-03,  2.99262740e-02,\n",
       "         3.89346518e-02, -1.24468803e-02,  4.53501008e-02,\n",
       "         2.91485675e-02, -1.88795812e-02]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([1,2,3,4,5,6,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Based KNowledge graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
