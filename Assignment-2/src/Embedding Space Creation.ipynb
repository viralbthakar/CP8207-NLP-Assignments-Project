{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  Embedding Space Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"An embedding is a relatively low-dimensional space into which you can translate high-dimensional vectors. Embeddings make it easier to do machine learning on large inputs like sparse vectors representing words. Ideally, an embedding captures some of the semantics of the input by placing semantically similar inputs close together in the embedding space. An embedding can be learned and reused across models.\" $-$ Machine Learning Crash Course with TensorFlow APIs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This Notebook is based on the official [Word2Vec Tutorial](https://www.tensorflow.org/tutorials/text/word2vec) from Tensorflow.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreating Word2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Word2Vec?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec [[Mikolov, Tomas, et al. 2013a](https://arxiv.org/pdf/1301.3781.pdf) and [Mikolov, Tomas, et al. 2013b](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)] is a popular natural language processing technique that is used to create high-quality vector representations of words from large datasets of text. It is a neural network based model that is capable of capturing the semantic and syntactic meaning of words, and it has been widely used in various downstream NLP tasks such as text classification, sentiment analysis, and machine translation. Word2Vec has revolutionized the field of NLP by providing a more efficient and effective way to analyze and understand natural language text. In this document, we will provide a comprehensive overview of Word2Vec, its architecture, and recreate Word2Vec for our custom dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Common Types of Methods for Word2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main types of methods used to create Word2Vec models:\n",
    "\n",
    "- **Continuous Bag of Words (CBOW)**: In this method, the model predicts the target word based on the context words that surround it. The context words are used as input to the model, and the output is the probability distribution of the target word given the context words.\n",
    "\n",
    "- **Skip-gram**: In this method, the model predicts the context words given a target word. The target word is used as input to the model, and the output is the probability distribution of the context words given the target word.\n",
    "\n",
    "Both methods use a neural network architecture with one hidden layer to learn the vector representations of the words. The size of the hidden layer determines the dimensionality of the word vectors, and typically ranges from a few hundred to a few thousand. The Word2Vec models are trained on large corpora of text data using stochastic gradient descent, and the resulting word vectors are used in various NLP applications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¥· As for our use case in this Assignment, we are interested to create Knowledge Graphs, Topic Modeling and Entity-Relationship extraction as downstream tasks, we find that `Skip-gram` approach will be much suitable for us as underline it is trying to predict the context for a given word, where we can consider context as neighboring words for a given word. This will be very useful to us in establishing the strong relationships between different words."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-gram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip-gram is a natural language processing technique used to create vector representations of words. As mentioned earlier, It is a type of Word2Vec model that learns to `predict the context words given a target word`. The basic idea behind Skip-gram is to use the target word as input to a neural network, and then predict the probability distribution of the context words that are likely to appear with the target word in a sentence.\n",
    "\n",
    "The Skip-gram model takes a corpus of text as input, and creates a vocabulary of all the unique words in the corpus. Each word is represented by a vector of a fixed dimensionality (e.g., 100, 200, or 300). The Skip-gram model then trains a neural network on this vocabulary using a sliding window approach.\n",
    "\n",
    "In this approach, a window of fixed size (e.g., 5) is moved across the text corpus, and for each target word in the window, the model is trained to predict the surrounding context words. This process is repeated for all target words in the corpus.\n",
    "\n",
    "During training, the model adjusts the vector representations of each word in the vocabulary based on the prediction errors. After training, the word vectors are used to represent the semantic and syntactic meaning of words, and can be used in various downstream NLP tasks such as sentiment analysis, text classification, and machine translation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are few examples of Skip-grams:\n",
    "\n",
    "- Consider the sentence **\"The quick brown fox jumps over the lazy dog\"**. Using a window size of 2, the Skip-gram model would generate training pairs like `(quick, The)`, `(quick, brown)`, `(brown, quick)`, `(brown, fox)`, `(fox, brown)`, and so on. The model learns to predict the context words (e.g., The, brown, fox) given a target word (e.g., quick).\n",
    "\n",
    "- Let's say we are training a Skip-gram model on a corpus of movie reviews. The model might learn that the word \"awesome\" tends to appear in the context of positive sentiment words like \"great\", \"fantastic\", and \"amazing\", while it is less likely to appear in the context of negative sentiment words like \"bad\", \"terrible\", and \"awful\". This information can then be used to perform sentiment analysis on new movie reviews.\n",
    "\n",
    "- Suppose we want to train a Skip-gram model to represent the semantic relationships between different animals. The model might learn that the vector representations of \"dog\" and \"cat\" are similar, while the vectors of \"dog\" and \"snake\" are dissimilar. This information can then be used to perform tasks such as animal classification or identification. **This example is very close to our use case in this Assignment** ðŸ¥·."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training objective of the Skip-gram model can be represented by the following negative log-likelihood function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$-\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{-c\\le j\\le c, j\\ne 0}\\log P(w_{t+j}\\mid w_t)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where \n",
    "- $T$ is the total number of words in the corpus, \n",
    "- $c$ is the size of the context window, \n",
    "- $w_t$ is the target word at position $t$, \n",
    "- $w_{t+j}$ is the context word $j$ positions away from the target word, \n",
    "- $P(w_{t+j}\\mid w_t)$ is the probability of the context word given the target word. \n",
    "\n",
    "The Skip-gram model aims to maximize this objective function by adjusting the vector representations of the words in the corpus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(w_{t+j}\\mid w_t)=\\frac{\\exp(\\mathbf{v}_{w{t+j}}\\cdot\\mathbf{v}_{w_t})}{\\sum_{i=1}^{V}\\exp(\\mathbf{v}_i\\cdot\\mathbf{v}_{w_t})}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where \n",
    "- $\\mathbf{v}_{w{t+j}}$ is the vector representation of the context word $w_{t+j}$, \n",
    "- $\\mathbf{v}_{w_t}$ is the vector representation of the target word $w_t$,\n",
    "- $V$ is the size of the vocabulary. \n",
    "\n",
    "The dot product of the two vectors measures the similarity between the target word and the context word, and the softmax function normalizes the probabilities of all the context words in the vocabulary. **The Skip-gram model learns to maximize the probability of the context words that are likely to appear with the target word in the corpus.** ðŸ¥·"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the denominator of this formulation involves performing a full softmax over the entire vocabulary words, which are often large ($10^5$ - $10^7$) terms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `noise contrastive estimation (NCE)` loss function provides a useful alternative to the full softmax in order to learn word embeddings. \n",
    "\n",
    "The objective of NCE loss is to distinguish context words from negative samples drawn from a noise distribution. This negative sampling can simplify the NCE loss for a target word by posing it as a classification problem between the context word and a certain number of negative samples. This provides an efficient approximation of the full softmax over the vocabulary in a skip-gram model.\n",
    "\n",
    "A `negative sample` is defined as a `(target word, context word)` pair such that the context word does not appear in the window size neighborhood of the target word. Let's say for **\"The quick brown fox jumps over the lazy dog\"** sentence we want to train a Skip-gram model with a context window of size 2. Given the target word \"fox\", one negative sample could be the word \"apple\" (`(fox, apple)`). We can draw this negative sample from a noise distribution that assigns low probabilities to words that are unlikely to appear in the context of the target word. In this case, \"apple\" is a word that is not likely to appear in the context of \"fox\", so it serves as a suitable negative sample. Another such example of negative sample could be `(fox, dog)`. Since \"dog\" is not likely to appear in the context of \"fox\" (in this sentence), it can be used as a negative sample. However, it is important to note that the number of negative samples chosen for the Skip-gram model depends on the size of the corpus and the context window, and a larger number of negative samples can result in a more stable and accurate model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Skip-grams using Tensorflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation of Skip-grams involves three main steps:\n",
    "1. Vectorize every sentence encoded as a list of word indices.\n",
    "    - Convert Sentence into Tokens.\n",
    "    - Create a vocabulary to save mappings from tokens to integer indices.\n",
    "    - Use vocabulary to vectorize every sentence in the dataset.\n",
    "2. Use `tf.keras.preprocessing.sequence.skipgrams` to create skipgrams.\n",
    "    - This function transforms a sequence of word indexes (list of integers) into tuples of words of the form:\n",
    "        - (word, word in the same window), with label 1 (positive samples).\n",
    "        - (word, random word from the vocabulary), with label 0 (negative samples).\n",
    "    - Provide a word sequence (sentence), encoded as a list of word indices (integers) as input.\n",
    "    - Provide `vocabulary size` and `window size` as input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "from utils import styled_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(sentence):\n",
    "    tokens = list(sentence.lower().split())\n",
    "    vocabulary = defaultdict(int)\n",
    "    vocabulary['<pad>'] = 0\n",
    "    index = 1\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token not in vocabulary:\n",
    "            vocabulary[token] = index\n",
    "            index += 1\n",
    "    inverse_vocabulary = {index: token for token, index in vocabulary.items()}\n",
    "    return tokens, vocabulary, inverse_vocabulary\n",
    "\n",
    "def vectorize_sentence(sentence, vocabulary):\n",
    "    tokens = list(sentence.lower().split())\n",
    "    sentence = [vocabulary[word] for word in tokens]\n",
    "    return sentence\n",
    "\n",
    "def print_skipgrams(skip_grams, labels, inverse_vocabulary, num_samples=5):\n",
    "    index = 0\n",
    "    if num_samples is None:\n",
    "        num_samples = len(skip_grams)\n",
    "    for target, context in skip_grams[:num_samples]:\n",
    "        styled_print(f\"({target}, {context}): ({inverse_vocabulary[target]}, {inverse_vocabulary[context]}) : Label {labels[index]}\")\n",
    "        index+=1\n",
    "\n",
    "def create_skip_gram(sentence, window_size=2, sampling_table=None, only_positive_skip_grams=True):\n",
    "    tokens, vocabulary, inverse_vocabulary = create_vocabulary(sentence)\n",
    "    styled_print(f\"Found {len(tokens)} Tokes: {tokens}\", header=False)\n",
    "    styled_print(f\"Vocabulary: {dict(vocabulary)}\", header=False)\n",
    "\n",
    "    word_sequence = vectorize_sentence(sentence, vocabulary)\n",
    "    styled_print(f\"Word Sequence: {word_sequence}\", header=False)\n",
    "\n",
    "    if only_positive_skip_grams:\n",
    "        negative_samples = 0\n",
    "    else:\n",
    "        negative_samples = 1\n",
    "\n",
    "    skip_grams, labels = tf.keras.preprocessing.sequence.skipgrams(\n",
    "        word_sequence,\n",
    "        vocabulary_size=len(vocabulary),\n",
    "        window_size=window_size,\n",
    "        sampling_table=sampling_table,\n",
    "        negative_samples=negative_samples)\n",
    "    styled_print(f\"Found Total {len(skip_grams)} skip grams\")\n",
    "    return skip_grams, labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generating Positive Skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mâ€º \u001b[4mCreating Skipgrams using Tensorflow\u001b[0m\n",
      "\u001b[1mâ€º \u001b[4mSome Samples of Positive Skip Grams Only\u001b[0m\n",
      "    â€º Found 9 Tokes: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "    â€º Vocabulary: {'<pad>': 0, 'the': 1, 'quick': 2, 'brown': 3, 'fox': 4, 'jumps': 5, 'over': 6, 'lazy': 7, 'dog': 8}\n",
      "    â€º Word Sequence: [1, 2, 3, 4, 5, 6, 1, 7, 8]\n",
      "    â€º Found Total 30 skip grams\n",
      "    â€º (4, 6): (fox, over) : Label 1\n",
      "    â€º (4, 2): (fox, quick) : Label 1\n",
      "    â€º (1, 3): (the, brown) : Label 1\n",
      "    â€º (8, 1): (dog, the) : Label 1\n",
      "    â€º (7, 8): (lazy, dog) : Label 1\n",
      "\u001b[1mâ€º \u001b[4mSome Samples of Positive and Negative Skip Grams\u001b[0m\n",
      "    â€º Found 9 Tokes: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "    â€º Vocabulary: {'<pad>': 0, 'the': 1, 'quick': 2, 'brown': 3, 'fox': 4, 'jumps': 5, 'over': 6, 'lazy': 7, 'dog': 8}\n",
      "    â€º Word Sequence: [1, 2, 3, 4, 5, 6, 1, 7, 8]\n",
      "    â€º Found Total 60 skip grams\n",
      "    â€º (3, 1): (brown, the) : Label 1\n",
      "    â€º (5, 8): (jumps, dog) : Label 0\n",
      "    â€º (1, 8): (the, dog) : Label 1\n",
      "    â€º (7, 5): (lazy, jumps) : Label 0\n",
      "    â€º (2, 7): (quick, lazy) : Label 0\n",
      "\u001b[1mâ€º \u001b[4mSome Samples of Positive and Negative Skip Grams with Window Size of 3\u001b[0m\n",
      "    â€º Found 9 Tokes: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "    â€º Vocabulary: {'<pad>': 0, 'the': 1, 'quick': 2, 'brown': 3, 'fox': 4, 'jumps': 5, 'over': 6, 'lazy': 7, 'dog': 8}\n",
      "    â€º Word Sequence: [1, 2, 3, 4, 5, 6, 1, 7, 8]\n",
      "    â€º Found Total 84 skip grams\n",
      "    â€º (1, 3): (the, brown) : Label 0\n",
      "    â€º (8, 6): (dog, over) : Label 1\n",
      "    â€º (4, 1): (fox, the) : Label 1\n",
      "    â€º (8, 6): (dog, over) : Label 0\n",
      "    â€º (3, 1): (brown, the) : Label 1\n"
     ]
    }
   ],
   "source": [
    "styled_print(\"Creating Skipgrams using Tensorflow\", header=True)\n",
    "styled_print(f\"Some Samples of Positive Skip Grams Only\", header=True)\n",
    "skip_grams, labels = create_skip_gram(sentence, window_size=2, only_positive_skip_grams=True)\n",
    "tokens, vocabulary, inverse_vocabulary = create_vocabulary(sentence)\n",
    "print_skipgrams(skip_grams, labels, inverse_vocabulary, 5)\n",
    "\n",
    "styled_print(f\"Some Samples of Positive and Negative Skip Grams\", header=True)\n",
    "skip_grams, labels = create_skip_gram(sentence, window_size=2, only_positive_skip_grams=False)\n",
    "tokens, vocabulary, inverse_vocabulary = create_vocabulary(sentence)\n",
    "print_skipgrams(skip_grams, labels, inverse_vocabulary, 5)\n",
    "\n",
    "styled_print(f\"Some Samples of Positive and Negative Skip Grams with Window Size of 3\", header=True)\n",
    "skip_grams, labels = create_skip_gram(sentence, window_size=3, only_positive_skip_grams=False)\n",
    "tokens, vocabulary, inverse_vocabulary = create_vocabulary(sentence)\n",
    "print_skipgrams(skip_grams, labels, inverse_vocabulary, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sampling Table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with large datasets, the vocabulary size tends to be bigger, with more frequently occurring words such as stopwords. However, using training examples from such commonly occurring words does not offer much useful information for the model to learn from. To address this, [Mikolov, Tomas, et al. 2013a](https://arxiv.org/pdf/1301.3781.pdf) and [Mikolov, Tomas, et al. 2013b](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf) have suggested that subsampling frequent words can improve the quality of word embeddings. A sampling table can be used to encode the probabilities of sampling any token in the training data. The `tf.keras.preprocessing.sequence.skipgrams` function can accept a sampling table as input, and the `tf.keras.preprocessing.sequence.make_sampling_table` function can generate a word-frequency rank based probabilistic sampling table that can be passed to the `tf.keras.preprocessing.sequence.skipgrams` function. One can inspect the sampling probabilities for a vocabulary size of 10 as follows where `sampling_table[i]` denotes the probability of sampling the i-th most common word in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    â€º [0.00315225 0.00315225 0.00547597 0.00741556 0.00912817 0.01068435\n",
      " 0.01212381 0.01347162 0.01474487 0.0159558 ]\n"
     ]
    }
   ],
   "source": [
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=10)\n",
    "styled_print(sampling_table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the most frequent words will have lease probability of sampling. Let's try to create sampling table for our vocabulary and the create skip grams based on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    â€º [0.09968283 0.09968283 0.17316546 0.23450073 0.288658   0.33786866\n",
      " 0.38338842 0.42601017 0.46627369]\n"
     ]
    }
   ],
   "source": [
    "tokens, vocabulary, inverse_vocabulary = create_vocabulary(sentence)\n",
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(len(vocabulary), sampling_factor=0.01)\n",
    "styled_print(sampling_table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are setting `sampling_factor=0.01` while the default value is `sampling_factor=1e-5`. The default value is much suitable for large vocabulary. As we have a small vocabulary we need to update it with a slightly larger number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mâ€º \u001b[4mCreating Skipgrams using Tensorflow\u001b[0m\n",
      "\u001b[1mâ€º \u001b[4mSome Samples of Positive Skip Grams Only\u001b[0m\n",
      "    â€º Found 9 Tokes: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "    â€º Vocabulary: {'<pad>': 0, 'the': 1, 'quick': 2, 'brown': 3, 'fox': 4, 'jumps': 5, 'over': 6, 'lazy': 7, 'dog': 8}\n",
      "    â€º Word Sequence: [1, 2, 3, 4, 5, 6, 1, 7, 8]\n",
      "    â€º Found Total 9 skip grams\n",
      "    â€º (5, 3): (jumps, brown) : Label 1\n",
      "    â€º (5, 1): (jumps, the) : Label 1\n",
      "    â€º (7, 8): (lazy, dog) : Label 1\n",
      "    â€º (7, 6): (lazy, over) : Label 1\n",
      "    â€º (8, 7): (dog, lazy) : Label 1\n",
      "\u001b[1mâ€º \u001b[4mSome Samples of Positive and Negative Skip Grams\u001b[0m\n",
      "    â€º Found 9 Tokes: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "    â€º Vocabulary: {'<pad>': 0, 'the': 1, 'quick': 2, 'brown': 3, 'fox': 4, 'jumps': 5, 'over': 6, 'lazy': 7, 'dog': 8}\n",
      "    â€º Word Sequence: [1, 2, 3, 4, 5, 6, 1, 7, 8]\n",
      "    â€º Found Total 12 skip grams\n",
      "    â€º (8, 7): (dog, lazy) : Label 1\n",
      "    â€º (8, 7): (dog, lazy) : Label 0\n",
      "    â€º (4, 5): (fox, jumps) : Label 1\n",
      "    â€º (4, 6): (fox, over) : Label 1\n",
      "    â€º (4, 3): (fox, brown) : Label 1\n",
      "\u001b[1mâ€º \u001b[4mSome Samples of Positive and Negative Skip Grams with Window Size of 3\u001b[0m\n",
      "    â€º Found 9 Tokes: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "    â€º Vocabulary: {'<pad>': 0, 'the': 1, 'quick': 2, 'brown': 3, 'fox': 4, 'jumps': 5, 'over': 6, 'lazy': 7, 'dog': 8}\n",
      "    â€º Word Sequence: [1, 2, 3, 4, 5, 6, 1, 7, 8]\n",
      "    â€º Found Total 0 skip grams\n"
     ]
    }
   ],
   "source": [
    "styled_print(\"Creating Skipgrams using Tensorflow\", header=True)\n",
    "styled_print(f\"Some Samples of Positive Skip Grams Only\", header=True)\n",
    "skip_grams, labels = create_skip_gram(sentence, window_size=2, sampling_table=sampling_table, only_positive_skip_grams=True)\n",
    "print_skipgrams(skip_grams, labels, inverse_vocabulary, 5)\n",
    "\n",
    "styled_print(f\"Some Samples of Positive and Negative Skip Grams\", header=True)\n",
    "skip_grams, labels = create_skip_gram(sentence, window_size=2, sampling_table=sampling_table, only_positive_skip_grams=False)\n",
    "print_skipgrams(skip_grams, labels, inverse_vocabulary, 5)\n",
    "\n",
    "styled_print(f\"Some Samples of Positive and Negative Skip Grams with Window Size of 3\", header=True)\n",
    "skip_grams, labels = create_skip_gram(sentence, window_size=3, sampling_table=sampling_table, only_positive_skip_grams=False)\n",
    "print_skipgrams(skip_grams, labels, inverse_vocabulary, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we should focus on the `total number of skipgrams found`. We can see that with `sampling_table` argument we have less number of skipgrams and that is because it is assigning less probabilities of selecting most frequent workds i.e. `the` in our example. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Negative Sampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `only_positive_skip_grams` argument will allow us to create negative samples same as number of positive skip grams in our data. It is a good feature is we would like to create a balance dataset. But we are interested to create more negative samples for each positive sample as it will help us to extend our dataset and will be useful for `noise contrastive estimation (NCE)` loss function. In the this part we create $N$ number of negative samples for a given target word. This will be an important step in our data pipeline for Word2Vec model training. For this purpose we will use `tf.random.log_uniform_candidate_sampler` function to sample `num_ns` words from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    â€º Found 9 Tokes: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "    â€º Vocabulary: {'<pad>': 0, 'the': 1, 'quick': 2, 'brown': 3, 'fox': 4, 'jumps': 5, 'over': 6, 'lazy': 7, 'dog': 8}\n",
      "    â€º Word Sequence: [1, 2, 3, 4, 5, 6, 1, 7, 8]\n",
      "    â€º Found Total 30 skip grams\n",
      "\u001b[1mâ€º \u001b[4mLet's sample negative candidates for (8, 7) - ('dog', 'lazy') pair\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-14 14:46:31.288194: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-03-14 14:46:31.288503: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n",
      "    â€º Fetched [7 1 3 4 0] indexes for negatives words\n",
      "    â€º ['lazy', 'the', 'brown', 'fox', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "def get_negative_sampling_candidates(context, num_ns, vocab_size, seed):\n",
    "    context_class = tf.reshape(tf.constant(context, dtype=\"int64\"), (1, 1))\n",
    "    negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "        true_classes=context_class,\n",
    "        num_true=1,\n",
    "        num_sampled=num_ns,\n",
    "        unique=True,\n",
    "        range_max=vocab_size,\n",
    "        seed=seed,\n",
    "        name=\"negative_sampling\"\n",
    "    )\n",
    "    return negative_sampling_candidates\n",
    "\n",
    "tokens, vocabulary, inverse_vocabulary = create_vocabulary(sentence)\n",
    "skip_grams, labels = create_skip_gram(sentence, window_size=2, only_positive_skip_grams=True)\n",
    "\n",
    "sample_target, sample_context = skip_grams[0]\n",
    "styled_print(f\"Let's sample negative candidates for {(sample_target, sample_context)} - {(inverse_vocabulary[sample_target], inverse_vocabulary[sample_context])} pair\", header=True)\n",
    "negative_sampling_candidates = get_negative_sampling_candidates(sample_context, 5, len(vocabulary), 1)\n",
    "styled_print(f\"Fetched {negative_sampling_candidates} indexes for negatives words\")\n",
    "styled_print([inverse_vocabulary[index.numpy()] for index in negative_sampling_candidates])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that the `negative_sampling_candidates` sometime also include our positive context class while we expect it to explicitly exclude the positive context class. This type of behavior is explained in [this](https://www.tensorflow.org/extras/candidate_sampling.pdf) document and [this](https://github.com/tensorflow/tensorflow/issues/44758#issuecomment-916554100) comment. It is not intuitive but the underline idea is that even though in this particular example a given `(target, context)` pair is part of positive skipgram but the same pair could be part of negative skipgram in some other data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create tf.data Datapipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>paragraphs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>he maesters of the Citadel who keep the histor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>either AC (After the Conquest) or BC (Before t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>True scholars know that such dating is far fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>Even the start date is a matter of some miscon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>106</td>\n",
       "      <td>battles of the Wars of Conquest had been fough...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                         paragraphs\n",
       "0  101  he maesters of the Citadel who keep the histor...\n",
       "1  102  either AC (After the Conquest) or BC (Before t...\n",
       "2  103  True scholars know that such dating is far fro...\n",
       "3  104  Even the start date is a matter of some miscon...\n",
       "4  106  battles of the Wars of Conquest had been fough..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_csv_file = \"../data/processed-data/clean-csvs/book-raw-paragraphs.csv\"\n",
    "book_df = pd.read_csv(book_csv_file)\n",
    "book_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraphs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>he maesters of the Citadel who keep the histor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>either AC (After the Conquest) or BC (Before t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True scholars know that such dating is far fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even the start date is a matter of some miscon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>battles of the Wars of Conquest had been fough...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          paragraphs\n",
       "0  he maesters of the Citadel who keep the histor...\n",
       "1  either AC (After the Conquest) or BC (Before t...\n",
       "2  True scholars know that such dating is far fro...\n",
       "3  Even the start date is a matter of some miscon...\n",
       "4  battles of the Wars of Conquest had been fough..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_df = book_df.drop([\"id\"], axis=1)\n",
    "book_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.data` pipelines are usually confusing to understand. In this notebook we will take step by step approach to explain and understand each step of our data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = tf.data.Dataset.from_tensor_slices(dict(book_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mâ€º \u001b[4mChecking first five Sample from the tf.data Datapipeline\u001b[0m\n",
      "    â€º 'paragraphs'   : b'he maesters of the Citadel who keep the histories of Westeros have used Aegon\\xe2\\x80\\x99s Conquest as their touchstone for the past three hundred years. Births, deaths, battles, and other events are dated'\n",
      "    â€º 'paragraphs'   : b'either AC (After the Conquest) or BC (Before the Conquest).'\n",
      "    â€º 'paragraphs'   : b'True scholars know that such dating is far from precise. Aegon Targaryen\\xe2\\x80\\x99s conquest of the Seven Kingdoms did not take place in a single day. More than two years passed between Aegon\\xe2\\x80\\x99s landing and his Oldtown coronation\\xe2\\x80\\xa6and even then the Conquest remained incomplete, since Dorne remained unsubdued. Sporadic attempts to bring the Dornishmen into the realm continued all through King Aegon\\xe2\\x80\\x99s reign and well into the reigns of his sons, making it impossible to fix a precise end date for the Wars of Conquest.'\n",
      "    â€º 'paragraphs'   : b'Even the start date is a matter of some misconception. Many assume, wrongly, that the reign of King Aegon I Targaryen began on the day he landed at the mouth of the Blackwater Rush, beneath the three hills where the city of King\\xe2\\x80\\x99s Landing would eventually stand. Not so. The day of Aegon\\xe2\\x80\\x99s Landing was celebrated by the king and his descendants, but the Conqueror actually dated the start of his reign from the day he was crowned and anointed in the Starry Sept of Oldtown by the High Septon of the Faith. This coronation took place two years after Aegon\\xe2\\x80\\x99s Landing, well after all three of the major'\n",
      "    â€º 'paragraphs'   : b'battles of the Wars of Conquest had been fought and won. Thus it can be seen that most of Aegon\\xe2\\x80\\x99s actual conquering took place from 2\\xe2\\x80\\x931 BC, Before the Conquest.'\n"
     ]
    }
   ],
   "source": [
    "styled_print(f\"Checking first five Sample from the tf.data Datapipeline\", header=True)\n",
    "for feature_batch in datagen.take(5):\n",
    "    for key, value in feature_batch.items():\n",
    "        styled_print(\"{!r:15s}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_sentence(sample):\n",
    "    return sample[\"paragraphs\"]\n",
    "datagen = datagen.map(only_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mâ€º \u001b[4mChecking first five Sample from the tf.data Datapipeline\u001b[0m\n",
      "    â€º b'he maesters of the Citadel who keep the histories of Westeros have used Aegon\\xe2\\x80\\x99s Conquest as their touchstone for the past three hundred years. Births, deaths, battles, and other events are dated'\n",
      "    â€º b'either AC (After the Conquest) or BC (Before the Conquest).'\n",
      "    â€º b'True scholars know that such dating is far from precise. Aegon Targaryen\\xe2\\x80\\x99s conquest of the Seven Kingdoms did not take place in a single day. More than two years passed between Aegon\\xe2\\x80\\x99s landing and his Oldtown coronation\\xe2\\x80\\xa6and even then the Conquest remained incomplete, since Dorne remained unsubdued. Sporadic attempts to bring the Dornishmen into the realm continued all through King Aegon\\xe2\\x80\\x99s reign and well into the reigns of his sons, making it impossible to fix a precise end date for the Wars of Conquest.'\n",
      "    â€º b'Even the start date is a matter of some misconception. Many assume, wrongly, that the reign of King Aegon I Targaryen began on the day he landed at the mouth of the Blackwater Rush, beneath the three hills where the city of King\\xe2\\x80\\x99s Landing would eventually stand. Not so. The day of Aegon\\xe2\\x80\\x99s Landing was celebrated by the king and his descendants, but the Conqueror actually dated the start of his reign from the day he was crowned and anointed in the Starry Sept of Oldtown by the High Septon of the Faith. This coronation took place two years after Aegon\\xe2\\x80\\x99s Landing, well after all three of the major'\n",
      "    â€º b'battles of the Wars of Conquest had been fought and won. Thus it can be seen that most of Aegon\\xe2\\x80\\x99s actual conquering took place from 2\\xe2\\x80\\x931 BC, Before the Conquest.'\n"
     ]
    }
   ],
   "source": [
    "styled_print(\n",
    "    f\"Checking first five Sample from the tf.data Datapipeline\", header=True)\n",
    "for feature_batch in datagen.take(5):\n",
    "    styled_print(feature_batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we actually create batch of data for model training, we need to create skipgrams. Previously we have created a custom function to vectorize the sentence. In this section as we have a bigger dataset, instead of using that custom function, we are using `tf.keras.layers.TextVectorization` layer to vectorize our dataset and create the vocabulary for our dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-14 14:46:38.716832: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-03-14 14:46:38.716970: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=None,\n",
    "    standardize='strip_punctuation',\n",
    "    split='whitespace',\n",
    "    ngrams=None,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=None,\n",
    "    pad_to_max_tokens=False\n",
    ")\n",
    "vectorize_layer.adapt(datagen.batch(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mâ€º \u001b[4mCreate Vocabulary and Vectorizer\u001b[0m\n",
      "    â€º The size of our Vocabulary is 15890\n",
      "    â€º First 10 tokens from our vocabulary are ['', '[UNK]', 'the', 'and', 'of', 'to', 'a', 'his', 'was', 'had', 'in', 'her', 'that', 'as', 'he', 'for', 'with', 'The', 'Lord', 'not']\n"
     ]
    }
   ],
   "source": [
    "styled_print(\"Create Vocabulary and Vectorizer\", header=True)\n",
    "styled_print(f\"The size of our Vocabulary is {vectorize_layer.vocabulary_size()}\")\n",
    "styled_print(f\"First 10 tokens from our vocabulary are {vectorize_layer.get_vocabulary()[:20]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `get_vocabulary()` method returns the tokens sorted in descending order by their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vector_datagen = datagen.batch(1024).prefetch(\n",
    "    tf.data.AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mâ€º \u001b[4mChecking first Sample from the tf.data Datapipeline\u001b[0m\n",
      "    â€º The Length of Sequence is 223\n",
      "    â€º [   14   509     4     2   590    34   510     2  3751     4   216    38\n",
      "   822   213   613    13    21  9984    15     2   644   101   242    83\n",
      " 15668  2620  1776     3    98  1326    97  8277     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0]\n",
      "    â€º The Length of Sequence is 223\n",
      "    â€º [1010  121  429    2  613   55 5421 1293    2  613    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "styled_print(\n",
    "    f\"Checking first Sample from the tf.data Datapipeline\", header=True)\n",
    "for feature_batch in text_vector_datagen.take(2):\n",
    "    styled_print(f\"The Length of Sequence is {len(feature_batch)}\")\n",
    "    styled_print(feature_batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make two main observations here:\n",
    "1. The length of each vectorized sentence is 223. This is same as the longest sentence in our dataset.\n",
    "2. Each vectored sequence is padded with `0s` at the end to make sure that each sentence after vectorization has the same length. This is very useful for Neural Network training as it will help us to have a fix input dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mâ€º \u001b[4mCheck Mapping of Some Vectorized Indexes with Tokens (Words)\u001b[0m\n",
      "    â€º 14 --> he\n",
      "    â€º 509 --> maesters\n",
      "    â€º 4 --> of\n",
      "    â€º 2 --> the\n",
      "    â€º 590 --> Citadel\n",
      "    â€º 34 --> who\n",
      "    â€º 510 --> keep\n",
      "    â€º 2 --> the\n",
      "    â€º 3751 --> histories\n",
      "    â€º 4 --> of\n",
      "    â€º 216 --> Westeros\n",
      "    â€º 38 --> have\n",
      "    â€º 822 --> used\n",
      "    â€º 213 --> Aegonâ€™s\n",
      "    â€º 613 --> Conquest\n",
      "    â€º 13 --> as\n",
      "    â€º 21 --> their\n",
      "    â€º 9984 --> touchstone\n",
      "    â€º 15 --> for\n",
      "    â€º 2 --> the\n",
      "    â€º 644 --> past\n",
      "    â€º 101 --> three\n",
      "    â€º 242 --> hundred\n",
      "    â€º 83 --> years\n",
      "    â€º 15668 --> Births\n",
      "    â€º 2620 --> deaths\n",
      "    â€º 1776 --> battles\n",
      "    â€º 3 --> and\n",
      "    â€º 98 --> other\n",
      "    â€º 1326 --> events\n"
     ]
    }
   ],
   "source": [
    "styled_print(\n",
    "    f\"Check Mapping of Some Vectorized Indexes with Tokens (Words)\", header=True)\n",
    "for feature_batch in text_vector_datagen.take(1):\n",
    "    for token in feature_batch[:30]:\n",
    "        styled_print(f\"{token} --> {vectorize_layer.get_vocabulary()[token]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wrap everything we have discussed so far in one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-14 14:46:59.692814: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "def get_preprocessing_datapipeline(df, batch_size=512):\n",
    "    datagen = tf.data.Dataset.from_tensor_slices(dict(df))\n",
    "    datagen = datagen.map(only_sentence, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "        max_tokens=None,\n",
    "        standardize='strip_punctuation',\n",
    "        split='whitespace',\n",
    "        ngrams=None,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=None,\n",
    "        pad_to_max_tokens=False\n",
    "    )\n",
    "    vectorize_layer.adapt(datagen.batch(batch_size))\n",
    "    \n",
    "    text_vector_datagen = datagen.batch(1024).prefetch(\n",
    "        tf.data.AUTOTUNE).map(vectorize_layer, num_parallel_calls=tf.data.AUTOTUNE).unbatch()\n",
    "    return datagen, text_vector_datagen, vectorize_layer\n",
    "\n",
    "\n",
    "_, text_vector_datagen, vectorize_layer = get_preprocessing_datapipeline(\n",
    "    book_df)\n",
    "sequences = list(text_vector_datagen.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mâ€º \u001b[4mChecking first few Sequences\u001b[0m\n",
      "    â€º The Length of Sequence is 223\n",
      "    â€º [   14   509     4     2   590    34   510     2  3751     4   216    38\n",
      "   822   213   613    13    21  9984    15     2   644   101   242    83\n",
      " 15668  2620  1776     3    98  1326    97  8277     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "styled_print(\n",
    "    f\"Checking first few Sequences\", header=True)\n",
    "for sequence in sequences[:1]:\n",
    "    styled_print(f\"The Length of Sequence is {len(sequence)}\")\n",
    "    styled_print(sequence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a function which takes these sequences as input and creates skipgrams as training pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_skip_grams(sequence, vocabulary_size, window_size=2, sampling_table=None, only_positive_skip_grams=True):\n",
    "    if only_positive_skip_grams:\n",
    "        negative_samples = 0\n",
    "    else:\n",
    "        negative_samples = 1\n",
    "    skip_grams, labels = tf.keras.preprocessing.sequence.skipgrams(\n",
    "        sequence,\n",
    "        vocabulary_size=vocabulary_size,\n",
    "        window_size=window_size,\n",
    "        sampling_table=sampling_table,\n",
    "        negative_samples=negative_samples)\n",
    "    return skip_grams, labels\n",
    "\n",
    "def get_negative_sampling_candidates(context, num_ns, vocabulary_size, seed):\n",
    "    context_class = tf.reshape(tf.constant(context, dtype=\"int64\"), (1, 1))\n",
    "    negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "        true_classes=context_class,\n",
    "        num_true=1,\n",
    "        num_sampled=num_ns,\n",
    "        unique=True,\n",
    "        range_max=vocabulary_size,\n",
    "        seed=seed,\n",
    "        name=\"negative_sampling\"\n",
    "    )\n",
    "    return negative_sampling_candidates\n",
    "\n",
    "def create_training_pairs(sequences, window_size, num_ns, vocabulary_size, seed=1):\n",
    "    targets, contexts, labels = [], [], []\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(\n",
    "        size=vocabulary_size)\n",
    "    for sequence in tqdm.tqdm(sequences):\n",
    "        skip_grams, _ = create_skip_grams(\n",
    "            sequence, \n",
    "            vocabulary_size, \n",
    "            window_size, \n",
    "            sampling_table, \n",
    "            only_positive_skip_grams=True\n",
    "        )\n",
    "        \n",
    "        for target_word, context_word in skip_grams:\n",
    "            negative_sampling_candidates = get_negative_sampling_candidates(\n",
    "                context_word, num_ns, vocabulary_size, seed \n",
    "            )\n",
    "            \n",
    "            # Build context and label vectors (for one target word)\n",
    "            context = tf.concat(\n",
    "                [tf.constant([context_word], dtype=\"int64\"), negative_sampling_candidates], 0)\n",
    "            label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "            # Append each element from the training example to global lists.\n",
    "            targets.append(target_word)\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "    return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3129/3129 [04:01<00:00, 12.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets.shape: (184617,)\n",
      "contexts.shape: (184617, 5)\n",
      "labels.shape: (184617, 5)\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = create_training_pairs(\n",
    "    sequences=sequences,\n",
    "    window_size=2,\n",
    "    num_ns=4,\n",
    "    vocabulary_size=vectorize_layer.vocabulary_size(),\n",
    "    seed=1)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_ns):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = tf.keras.layers.Embedding(vocab_size,\n",
    "                                             embedding_dim,\n",
    "                                             input_length=1,\n",
    "                                             name=\"w2v_embedding\")\n",
    "        self.context_embedding = tf.keras.layers.Embedding(vocab_size,\n",
    "                                              embedding_dim,\n",
    "                                              input_length=num_ns+1)\n",
    "    \n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "        # # context: (batch, context)\n",
    "        if len(target.shape) == 2:\n",
    "            target = tf.squeeze(target, axis=1)\n",
    "        # target: (batch,)\n",
    "        word_emb = self.target_embedding(target)\n",
    "        # word_emb: (batch, embed)\n",
    "        context_emb = self.context_embedding(context)\n",
    "        # context_emb: (batch, context, embed)\n",
    "        dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "        # dots: (batch, context)\n",
    "        return dots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 512\n",
    "# word2vec = Word2Vec(vectorize_layer.vocabulary_size(), embedding_dim, 4)\n",
    "word2vec = Word2Vec(10000, 128, 4)\n",
    "\n",
    "\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(\n",
    "                     from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  1/180 [..............................] - ETA: 1:21 - loss: 1.6096 - accuracy: 0.3154"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-14 14:51:52.948605: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 5s 28ms/step - loss: 1.6010 - accuracy: 0.3711\n",
      "Epoch 2/20\n",
      "180/180 [==============================] - 4s 21ms/step - loss: 1.5085 - accuracy: 0.5228\n",
      "Epoch 3/20\n",
      "180/180 [==============================] - 4s 21ms/step - loss: 1.4002 - accuracy: 0.5495\n",
      "Epoch 4/20\n",
      "180/180 [==============================] - 4s 21ms/step - loss: 1.2910 - accuracy: 0.6319\n",
      "Epoch 5/20\n",
      "180/180 [==============================] - 4s 22ms/step - loss: 1.1809 - accuracy: 0.7017\n",
      "Epoch 6/20\n",
      "180/180 [==============================] - 4s 21ms/step - loss: 1.0764 - accuracy: 0.7521\n",
      "Epoch 7/20\n",
      "180/180 [==============================] - 4s 21ms/step - loss: 0.9802 - accuracy: 0.7908\n",
      "Epoch 8/20\n",
      "180/180 [==============================] - 4s 22ms/step - loss: 0.8930 - accuracy: 0.8209\n",
      "Epoch 9/20\n",
      "180/180 [==============================] - 4s 21ms/step - loss: 0.8153 - accuracy: 0.8454\n",
      "Epoch 10/20\n",
      "180/180 [==============================] - 4s 21ms/step - loss: 0.7468 - accuracy: 0.8656\n",
      "Epoch 11/20\n",
      "180/180 [==============================] - 4s 21ms/step - loss: 0.6869 - accuracy: 0.8830\n",
      "Epoch 12/20\n",
      "180/180 [==============================] - 4s 21ms/step - loss: 0.6350 - accuracy: 0.8979\n",
      "Epoch 13/20\n",
      "180/180 [==============================] - 4s 22ms/step - loss: 0.5900 - accuracy: 0.9106\n",
      "Epoch 14/20\n",
      "180/180 [==============================] - 4s 21ms/step - loss: 0.5511 - accuracy: 0.9217\n",
      "Epoch 15/20\n",
      "180/180 [==============================] - 4s 21ms/step - loss: 0.5175 - accuracy: 0.9314\n",
      "Epoch 16/20\n",
      "180/180 [==============================] - 4s 21ms/step - loss: 0.4884 - accuracy: 0.9390\n",
      "Epoch 17/20\n",
      "180/180 [==============================] - 4s 21ms/step - loss: 0.4630 - accuracy: 0.9460\n",
      "Epoch 18/20\n",
      "180/180 [==============================] - 4s 21ms/step - loss: 0.4410 - accuracy: 0.9515\n",
      "Epoch 19/20\n",
      "180/180 [==============================] - 4s 21ms/step - loss: 0.4217 - accuracy: 0.9560\n",
      "Epoch 20/20\n",
      "180/180 [==============================] - 4s 21ms/step - loss: 0.4048 - accuracy: 0.9600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x177040c70>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"word2_vec\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " w2v_embedding (Embedding)   multiple                  1280000   \n",
      "                                                                 \n",
      " embedding (Embedding)       multiple                  1280000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,560,000\n",
      "Trainable params: 2,560,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(word2vec.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
