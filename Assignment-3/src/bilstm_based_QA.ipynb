{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 12:59:31.711882: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-04 12:59:31.871441: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-04 12:59:31.871464: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-04 12:59:31.904210: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-04-04 12:59:33.168897: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-04 12:59:33.169003: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-04 12:59:33.169018: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Attention\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task definition: \n",
    "GOAL: Create a biLSTM model to take context, and question, and generate an answer\n",
    "INPUT: Two inputs, each into their own biLSTM, one: \"CONTEXT\", two: \"QUESTION\"\n",
    "OUTPUT: [[START_POSITION], [END_POSITION]] one-hot-encoded with respect to the context\n",
    "1. Load SQuAD dataset\n",
    "2. Clean Context, and Question input\n",
    "3. Create input, and output lists\n",
    "4. Tokenize data\n",
    "5. Pad data\n",
    "6. Create one-hot-encoding of ANSWERS\n",
    "7. Define Model which consists of two inputs, two outputs, embedding layers, and bilstm layers\n",
    "6. Create a loss function to get the correct start and end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_json(\"../Data/SQuAD2/train-v2.0.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_POC = raw_data[\"data\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to create a model that takes two inputs: Context, Question as padded sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CQA_extraction_twoInputs(data):\n",
    "    context = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for topic in data:\n",
    "        for id, cq in enumerate(topic[\"paragraphs\"]):\n",
    "            for x in cq[\"qas\"]:\n",
    "                if x[\"is_impossible\"]==True:\n",
    "                    continue\n",
    "                question_text = x['question']\n",
    "                context_text = cq['context']\n",
    "\n",
    "                context.append(context_text)\n",
    "\n",
    "                questions.append(question_text)\n",
    "                \n",
    "                answer_text = x[\"answers\"][0][\"text\"]\n",
    "                answer_start = x[\"answers\"][0][\"answer_start\"]\n",
    "                \n",
    "                answers.append({\"text\":answer_text, \"start\":answer_start, \"end\":answer_start+len(answer_text), \"context\":context_text})#[f\"{x['question']}\"] = \n",
    "    return context, questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "context, questions, answers = CQA_extraction_twoInputs(testing_POC.head(20))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the Questions, and Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_context = [clean_text(x) for x in context]\n",
    "cleaned_questions = [clean_text(x) for x in questions]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also find the max length of the context!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3076"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = max([len(answer[\"context\"]) for answer in answers])\n",
    "context_length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define tokenizer\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit Tokenizer\n",
    "tokenizer.fit_on_texts(cleaned_questions+cleaned_context)\n",
    "#Create tokenized sequences of the context, and questions\n",
    "sequences_question = tokenizer.texts_to_sequences(cleaned_questions)\n",
    "sequences_context = tokenizer.texts_to_sequences(cleaned_context)\n",
    "#Find max sequence length of questions and context, together, and seperately\n",
    "#Max's are for padding\n",
    "max_length = max([len(x) for x in sequences_context+sequences_question])\n",
    "max_length_questions = max([len(x) for x in sequences_question])\n",
    "max_length_context = max([len(x) for x in sequences_context])\n",
    "#Pad sequences!\n",
    "padded_sequences_context = pad_sequences(sequences_context, maxlen=max_length_context, padding='post')\n",
    "padded_sequences_questions = pad_sequences(sequences_question, maxlen=max_length_questions, padding='post')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add padded sequences to X set\n",
    "X = []\n",
    "for id, x in enumerate(padded_sequences_questions):\n",
    "    X.append([padded_sequences_context[id], x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a vector of length conttext\n",
    "y_startPOS = [np.zeros(context_length) for answer in answers]\n",
    "y_endPOS = [np.zeros(context_length) for answer in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the start and end of each answer\n",
    "start_ends = [[answer[\"start\"], answer[\"end\"]] for answer in answers]\n",
    "#Map the start and end of each question to its position in\n",
    "#it's respective vector of length CONTEXT_LENGTH\n",
    "for id, vector in enumerate(y_startPOS):\n",
    "    vector[start_ends[id][0]]=1\n",
    "for id, vector in enumerate(y_endPOS):\n",
    "    vector[start_ends[id][0]]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = []\n",
    "for id, vector in enumerate(y_startPOS):\n",
    "    output_data.append([vector, y_endPOS[id]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, output_data, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5729, 3076)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np.array([np.array(y_train)[:,0], np.array(y_train)[:,0]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(context_sequence_length, question_sequence_length, vocab_length):\n",
    "    input_1 = tf.keras.layers.Input(shape=(context_sequence_length,))  # shape of input CONTEXT\n",
    "    input_2 = tf.keras.layers.Input(shape=(question_sequence_length,))  # shape of input QUESTION\n",
    "    #INPUT 1\n",
    "    #Embedding\n",
    "    embedding_1 = Embedding(input_dim=vocab_length, output_dim=100)(input_1)\n",
    "    #LSTM\n",
    "    lstm_1 = tf.keras.layers.Bidirectional(LSTM(units=64))(embedding_1)\n",
    "    #INPUT 2\n",
    "    #Embedding\n",
    "    embedding_2 = Embedding(input_dim=vocab_length, output_dim=100)(input_2)\n",
    "    #LSTM\n",
    "    lstm_2 = tf.keras.layers.Bidirectional(LSTM(units=64))(embedding_2)\n",
    "    #concat the layers\n",
    "    concatenated = tf.keras.layers.concatenate([lstm_1, lstm_2])\n",
    "    #Reshape\n",
    "    reshape_layer = tf.keras.layers.Reshape((256, 1))(concatenated)\n",
    "    #Dense layer \n",
    "    final_bilstm = tf.keras.layers.Bidirectional(LSTM(units=32))(reshape_layer)\n",
    "    output_start = Dense(units=3076, activation='softmax')(final_bilstm)\n",
    "    output_end = Dense(units=3076, activation='softmax')(final_bilstm)\n",
    "    #Define Model\n",
    "    goal2model = tf.keras.models.Model(inputs=[input_1, input_2], outputs=[output_start, output_end])\n",
    "\n",
    "    #Copile Model\n",
    "    goal2model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return goal2model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create model\n",
    "model = create_model(max_length_context, max_length_questions, len(tokenizer.word_index)+1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "180/180 [==============================] - 87s 435ms/step - loss: 14.3439 - dense_8_loss: 7.1717 - dense_9_loss: 7.1722 - dense_8_accuracy: 0.0213 - dense_9_accuracy: 0.0218\n",
      "Epoch 2/3\n",
      "180/180 [==============================] - 86s 478ms/step - loss: 13.1663 - dense_8_loss: 6.5831 - dense_9_loss: 6.5832 - dense_8_accuracy: 0.0248 - dense_9_accuracy: 0.0248\n",
      "Epoch 3/3\n",
      "180/180 [==============================] - 88s 488ms/step - loss: 13.0886 - dense_8_loss: 6.5444 - dense_9_loss: 6.5442 - dense_8_accuracy: 0.0248 - dense_9_accuracy: 0.0248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3e06964820>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([np.array([X[0] for X in X_train]), np.array([X[1] for X in X_train])], \n",
    "          [np.array(y_train)[:,0], np.array(y_train)[:,1]],epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 2s 34ms/step\n"
     ]
    }
   ],
   "source": [
    "l = model.predict([np.array([X[0] for X in X_test]), np.array([X[1] for X in X_test])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
      "\u001b[0;32m/tmp/ipykernel_91978/4221198456.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "np.argmax(l[3][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
